
@article{quan_coding_2005,
  title = {Coding {{Algorithms}} for {{Defining Comorbidities}} in {{ICD}}-9-{{CM}} and {{ICD}}-10 {{Administrative Data}}},
  volume = {43},
  copyright = {Copyright \textcopyright{} 2005 Lippincott Williams \& Wilkins},
  issn = {0025-7079},
  abstract = {Objectives: Implementation of the International Statistical Classification of Disease and Related Health Problems, 10th Revision (ICD-10) coding system presents challenges for using administrative data. Recognizing this, we conducted a multistep process to develop ICD-10 coding algorithms to define Charlson and Elixhauser comorbidities in administrative data and assess the performance of the resulting algorithms. Methods: ICD-10 coding algorithms were developed by "translation" of the ICD-9-CM codes constituting Deyo's (for Charlson comorbidities) and Elixhauser's coding algorithms and by physicians' assessment of the face-validity of selected ICD-10 codes. The process of carefully developing ICD-10 algorithms also produced modified and enhanced ICD-9-CM coding algorithms for the Charlson and Elixhauser comorbidities. We then used data on in-patients aged 18 years and older in ICD-9-CM and ICD-10 administrative hospital discharge data from a Canadian health region to assess the comorbidity frequencies and mortality prediction achieved by the original ICD-9-CM algorithms, the enhanced ICD-9-CM algorithms, and the new ICD-10 coding algorithms. Results: Among 56,585 patients in the ICD-9-CM data and 58,805 patients in the ICD-10 data, frequencies of the 17 Charlson comorbidities and the 30 Elixhauser comorbidities remained generally similar across algorithms. The new ICD-10 and enhanced ICD-9-CM coding algorithms either matched or outperformed the original Deyo and Elixhauser ICD-9-CM coding algorithms in predicting in-hospital mortality. The C-statistic was 0.842 for Deyo's ICD-9-CM coding algorithm, 0.860 for the ICD-10 coding algorithm, and 0.859 for the enhanced ICD-9-CM coding algorithm, 0.868 for the original Elixhauser ICD-9-CM coding algorithm, 0.870 for the ICD-10 coding algorithm and 0.878 for the enhanced ICD-9-CM coding algorithm. Conclusions: These newly developed ICD-10 and ICD-9-CM comorbidity coding algorithms produce similar estimates of comorbidity prevalence in administrative data, and may outperform existing ICD-9-CM coding algorithms.},
  number = {11},
  journal = {Medical Care},
  author = {Quan, Hude and Sundararajan, Vijaya and Halfon, Patricia and Fong, Andrew and Burnand, Bernard and Luthi, Jean-Christophe and Saunders, L. Duncan and Beck, Cynthia A. and Feasby, Thomas E. and Ghali, William A.},
  month = nov,
  year = {2005},
  pages = {1130-1139}
}

@article{elixhauser_comorbidity_1998,
  title = {Comorbidity {{Measures}} for {{Use}} with {{Administrative Data}}},
  volume = {36},
  issn = {0025-7079},
  abstract = {Objectives. This study attempts to develop a comprehensive set of comorbidity measures for use with large administrative inpatient datasets., Methods. The study involved clinical and empirical review of comorbidity measures, development of a framework that attempts to segregate comorbidities from other aspects of the patient's condition, development of a comorbidity algorithm, and testing on heterogeneous and homogeneous patient groups. Data were drawn from all adult, nonmaternal inpatients from 438 acute care hospitals in California in 1992 (n = 1,779,167). Outcome measures were those commonly available in administrative data: length of stay, hospital charges, and in-hospital death., Results. A comprehensive set of 30 comorbidity measures was developed. The comorbidities were associated with substantial increases in length of stay, hospital charges, and mortality both for heterogeneous and homogeneous disease groups. Several comorbidities are described that are important predictors of outcomes, yet commonly are not measured. These include mental disorders, drug and alcohol abuse, obesity, coagulopathy, weight loss, and fluid and electrolyte disorders., Conclusions. The comorbidities had independent effects on outcomes and probably should not be simplified as an index because they affect outcomes differently among different patient groups. The present method addresses some of the limitations of previous measures. It is based on a comprehensive approach to identifying comorbidities and separates them from the primary reason for hospitalization, resulting in an expanded set of comorbidities that easily is applied without further refinement to administrative data for a wide range of diseases., (C) Lippincott-Raven Publishers},
  number = {1},
  journal = {Medical Care January 1998},
  author = {Elixhauser, Anne and Steiner, Claudia and Harris, D. Robert and Coffey, Rosanna M.},
  year = {1998},
  keywords = {Clinical Medicine; Behavioral \& Social Sciences},
  pages = {8-27}
}

@article{quan_updating_2011,
  title = {Updating and {{Validating}} the {{Charlson Comorbidity Index}} and {{Score}} for {{Risk Adjustment}} in {{Hospital Discharge Abstracts Using Data From}} 6 {{Countries}}},
  volume = {173},
  issn = {0002-9262, 1476-6256},
  doi = {10.1093/aje/kwq433},
  abstract = {With advances in the effectiveness of treatment and disease management, the contribution of chronic comorbid diseases (comorbidities) found within the Charlson comorbidity index to mortality is likely to have changed since development of the index in 1984. The authors reevaluated the Charlson index and reassigned weights to each condition by identifying and following patients to observe mortality within 1 year after hospital discharge. They applied the updated index and weights to hospital discharge data from 6 countries and tested for their ability to predict in-hospital mortality. Compared with the original Charlson weights, weights generated from the Calgary, Alberta, Canada, data (2004) were 0 for 5 comorbidities, decreased for 3 comorbidities, increased for 4 comorbidities, and did not change for 5 comorbidities. The C statistics for discriminating in-hospital mortality between the new score generated from the 12 comorbidities and the Charlson score were 0.825 (new) and 0.808 (old), respectively, in Australian data (2008), 0.828 and 0.825 in Canadian data (2008), 0.878 and 0.882 in French data (2004), 0.727 and 0.723 in Japanese data (2008), 0.831 and 0.836 in New Zealand data (2008), and 0.869 and 0.876 in Swiss data (2008). The updated index of 12 comorbidities showed good-to-excellent discrimination in predicting in-hospital mortality in data from 6 countries and may be more appropriate for use with more recent administrative data.},
  language = {en},
  number = {6},
  journal = {American Journal of Epidemiology},
  author = {Quan, Hude and Li, Bing and Couris, Chantal M. and Fushimi, Kiyohide and Graham, Patrick and Hider, Phil and Januel, Jean-Marie and Sundararajan, Vijaya},
  month = mar,
  year = {2011},
  keywords = {Comorbidity,mortality,risk adjustment,Quality of Health Care,International Classification of Diseases},
  pages = {676-682},
  pmid = {21330339}
}

@article{charlson_new_1987,
  title = {A New Method of Classifying Prognostic Comorbidity in Longitudinal Studies: {{Development}} and Validation},
  volume = {40},
  issn = {0021-9681},
  shorttitle = {A New Method of Classifying Prognostic Comorbidity in Longitudinal Studies},
  doi = {10.1016/0021-9681(87)90171-8},
  abstract = {The objective of this study was to develop a prospectively applicable method for classifying comorbid conditions which might alter the risk of mortality for use in longitudinal studies. A weighted index that takes into account the number and the seriousness of comorbid disease was developed in a cohort of 559 medical patients. The 1-yr mortality rates for the different scores were: ``0'', 12\% (181); ``1\textendash{}2'', 26\% (225); ``3\textendash{}4'', 52\% (71); and ``$\geqslant$ 5'', 85\% (82). The index was tested for its ability to predict risk of death from comorbid disease in the second cohort of 685 patients during a 10-yr follow-up. The percent of patients who died of comorbid disease for the different scores were: ``0'', 8\% (588); ``1'', 25\% (54); ``2'', 48\% (25); `` $\geqslant$ 3'', 59\% (18). With each increased level of the comorbidity index, there were stepwise increases in the cumulative mortality attributable to comorbid disease (log rank $\chi$2 = 165; p \&lt; 0.0001). In this longer follow-up, age was also a predictor of mortality (p \&lt; 0.001). The new index performed similarly to a previous system devised by Kaplan and Feinstein. The method of classifying comorbidity provides a simple, readily applicable and valid method of estimating risk of death from comorbid disease for use in longitudinal studies. Further work in larger populations is still required to refine the approach because the number of patients with any given condition in this study was relatively small.},
  number = {5},
  journal = {Journal of Chronic Diseases},
  author = {Charlson, Mary E. and Pompei, Peter and Ales, Kathy L. and MacKenzie, C. Ronald},
  year = {1987},
  pages = {373-383}
}

@misc{r_development_core_team_r_????,
  title = {R {{Development Core Team}} (2013). {{R}}: {{A}} Language and Environment for Statistical Computing. {{R Foundation}} for {{Statistical Computing}}, {{Vienna}}, {{Austria}}. {{ISBN}} 3-900051-07-0, {{URL}} {{http://www.R-project.org.}}},
  publisher = {{R Foundation for Statistical Computing, Vienna, Austria}},
  author = {{R Development Core Team}}
}

@article{Eddelbeuttel_FastElegantNumerical_2013,
  title = {Fast and {{Elegant Numerical Linear Algebra Using}} the {{RcppEigen Package}} | {{Bates}} | {{Journal}} of {{Statistical Software}}},
  volume = {52},
  doi = {10.18637/jss.v052.i05},
  language = {en-US},
  number = {5},
  journal = {Journal of Statistical Software},
  author = {Eddelbeuttel, Dirk and Bates, Douglas},
  year = {2013}
}

@misc{Guennebaud_Eigen3_2017,
  title = {Eigen3},
  abstract = {Eigen is versatile.
        It supports all matrix sizes, from small fixed-size matrices to arbitrarily large dense matrices, and even sparse matrices.
        It supports all standard numeric types, including std::complex, integers, and is easily extensible to custom numeric types.
        It supports various matrix decompositions and geometry features.
        Its ecosystem of unsupported modules provides many specialized features such as non-linear optimization, matrix functions, a polynomial solver, FFT, and much more.
    Eigen is fast.
        Expression templates allow to intelligently remove temporaries and enable lazy evaluation, when that is appropriate.
        Explicit vectorization is performed for SSE 2/3/4, AVX, FMA, AVX512, ARM NEON (32-bit and 64-bit), PowerPC AltiVec/VSX (32-bit and 64-bit) instruction sets, and now S390x SIMD (ZVector) with graceful fallback to non-vectorized code.
        Fixed-size matrices are fully optimized: dynamic memory allocation is avoided, and the loops are unrolled when that makes sense.
        For large matrices, special attention is paid to cache-friendliness.
    Eigen is reliable.
        Algorithms are carefully selected for reliability. Reliability trade-offs are clearly documented and extremely safe decompositions are available.
        Eigen is thoroughly tested through its own test suite (over 500 executables), the standard BLAS test suite, and parts of the LAPACK test suite.
    Eigen is elegant.
        The API is extremely clean and expressive while feeling natural to C++ programmers, thanks to expression templates.
        Implementing an algorithm on top of Eigen feels like just copying pseudocode.
    Eigen has good compiler support as we run our test suite against many compilers to guarantee reliability and work around any compiler bugs. Eigen also is standard C++98 and maintains very reasonable compilation times.},
  howpublished = {https://eigen.tuxfamily.org/index.php?title=Main\_Page},
  author = {Guennebaud, G and Jacob, B and {et al}},
  year = {2017}
}

@article{Eddelbuettel_RcppSeamlessIntegration_2011,
  title = {Rcpp: {{Seamless R}} and {{C}}++ {{Integration}} | {{Eddelbuettel}} | {{Journal}} of {{Statistical Software}}},
  volume = {40},
  shorttitle = {Rcpp},
  doi = {10.18637/jss.v040.i08},
  language = {en-US},
  number = {8},
  journal = {Journal of Statistical Software},
  author = {Eddelbuettel, Dirk and Francois, Romain},
  year = {2011}
}

@misc{_BetaElixhauserComorbidity_,
  title = {Beta {{Elixhauser Comorbidity Software}} for {{ICD}}-10-{{CM}}},
  howpublished = {https://www.hcup-us.ahrq.gov/toolssoftware/comorbidityicd10/comorbidity\_icd10.jsp}
}

@article{vanWalraven_modificationElixhausercomorbidity_2009,
  title = {A Modification of the {{Elixhauser}} Comorbidity Measures into a Point System for Hospital Death Using Administrative Data},
  volume = {47},
  issn = {1537-1948},
  doi = {10.1097/MLR.0b013e31819432e5},
  abstract = {BACKGROUND: Comorbidity measures are necessary to describe patient populations and adjust for confounding. In direct comparisons, studies have found the Elixhauser comorbidity system to be statistically slightly superior to the Charlson comorbidity system at adjusting for comorbidity. However, the Elixhauser classification system requires 30 binary variables, making its use for reporting and analysis of comorbidity cumbersome.
OBJECTIVE: Modify the Elixhauser classification system into a single numeric score for administrative data.
METHODS: For all hospitalizations at the Ottawa Hospital, Canada, between 1996 and 2008, we determined if International Classification of Disease codes for chronic diagnoses were in any of the 30 Elixhauser comorbidity groups. We then used backward stepwise multivariate logistic regression to determine the independent association of each comorbidity group with death in hospital. Regression coefficients were modified into a scoring system that reflected the strength of each comorbidity group's independent association with hospital death.
RESULTS: Hospitalizations that were included were 345,795 (derivation: 228,565; validation 117,230). Twenty-one of the 30 groups were independently associated with hospital mortality. The resulting comorbidity score had an equivalent discrimination in the derivation and validation groups (overall c-statistic 0.763, 95\% CI: 0.759-0.766). This was similar to models having all Elixhauser groups (0.760, 95\% CI: 0.756-0.764) or significant groups only (0.759, 95\% CI: 0.754-0.762), but significantly exceeded discrimination when comorbidity was expressed using the Charlson score (0.745, 95\% CI: 0.742-0.749).
CONCLUSION: When analyzing administrative data, the Elixhauser comorbidity system can be condensed to a single numeric score that summarizes disease burden and is adequately discriminative for death in hospital.},
  language = {eng},
  number = {6},
  journal = {Medical Care},
  author = {{van Walraven}, Carl and Austin, Peter C. and Jennings, Alison and Quan, Hude and Forster, Alan J.},
  month = jun,
  year = {2009},
  keywords = {Cohort Studies,Comorbidity,Hospital Mortality,Humans,Hospitals; Teaching,Models; Statistical,Health Services Research,International Classification of Diseases,Hospital Administration,Risk Adjustment},
  pages = {626-633},
  pmid = {19433995}
}

@misc{Sutter_Fixingvector_2007,
  title = {Fixing Vector},
  howpublished = {http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2160.html},
  author = {Sutter, Herb},
  month = jan,
  year = {2007}
}

@inproceedings{Jarvi_AlgorithmSpecializationGeneric_2006,
  address = {New York, NY, USA},
  series = {PLDI '06},
  title = {Algorithm {{Specialization}} in {{Generic Programming}}: {{Challenges}} of {{Constrained Generics}} in {{C}}++},
  isbn = {978-1-59593-320-1},
  shorttitle = {Algorithm {{Specialization}} in {{Generic Programming}}},
  doi = {10.1145/1133981.1134014},
  abstract = {Generic programming has recently emerged as a paradigm for developing highly reusable software libraries, most notably in C++. We have designed and implemented a constrained generics extension for C++ to support modular type checking of generic algorithms and to address other issues associated with unconstrained generics. To be as broadly applicable as possible, generic algorithms are defined with minimal requirements on their inputs. At the same time, to achieve a high degree of efficiency, generic algorithms may have multiple implementations that exploit features of specific classes of inputs. This process of algorithm specialization relies on non-local type information and conflicts directly with the local nature of modular type checking. In this paper, we review the design and implementation of our extensions for generic programming in C++, describe the issues of algorithm specialization and modular type checking in detail, and discuss the important design tradeoffs in trying to accomplish both.We present the particular design that we chose for our implementation, with the goal of hitting the sweet spot in this interesting design space.},
  booktitle = {Proceedings of the 27th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {J{\"a}rvi, Jaakko and Gregor, Douglas and Willcock, Jeremiah and Lumsdaine, Andrew and Siek, Jeremy},
  year = {2006},
  keywords = {concepts,constrained generics,generic programming,parametric polymorphism,specialization},
  pages = {272--282}
}


